# Proyecto de Procesamiento de Lenguaje Natural: Word Embeddings y Generaci√≥n Literaria

Este repositorio contiene una implementaci√≥n avanzada de **Procesamiento de Lenguaje Natural (NLP)**, desarrollada en la **Universidad Nacional de Hurlingham (UNAHUR)**. 

El proyecto demuestra el flujo completo desde la creaci√≥n de representaciones sem√°nticas hasta la generaci√≥n autom√°tica de texto con estilo literario.

---

## üöÄ Estructura del Proyecto (3 Fases)

El repositorio est√° organizado siguiendo el flujo l√≥gico del desarrollo:

### 1Ô∏è‚É£ Fase 1: Word Embeddings (`01_Word_Embeddings/`)
Implementaci√≥n de los modelos **CBOW** y **SkipGram** con *Negative Sampling*. 
*   **Prop√≥sito**: Crear vectores densos que capturen el significado de las palabras bas√°ndose en su contexto.
*   **Contenido**: Scripts de entrenamiento y utilitarios para la l√≥gica de Word2Vec.

### 2Ô∏è‚É£ Fase 2: Entrenamiento de Redes Multicapa - TP2 (`02_Generacion_Texto/`)
Uso de los embeddings obtenidos en la Fase 1 para entrenar modelos predictivos complejos. Esta carpeta representa el n√∫cleo del **Trabajo Pr√°ctico 2**.
*   **Prop√≥sito**: Entrenar una red neuronal multicapa capaz de modelar el lenguaje de **Julio Cort√°zar**.
*   **Contenido**: Notebooks de entrenamiento, scripts de arquitectura y una consola de pruebas integrada.

### 3Ô∏è‚É£ Fase 3: Julio Cort√°zar GPT - Aplicaci√≥n Final (`03_Consola_Cortazar_GPT/`)
El producto final del proyecto: una interfaz gr√°fica interactiva independiente.
*   **Prop√≥sito**: Generar texto en tiempo real utilizando el mejor modelo consolidado.
*   **Contenido**: Consola interactiva (`consola.py`) y motor de predicci√≥n optimizado.

---

## üìÇ Organizaci√≥n de Recursos Adicionales

*   üìÇ **`models/`**: Pesos y par√°metros de los modelos entrenados en todas las fases.
*   üìÇ **`data/`**: Corpus ling√º√≠sticos (textos de Cort√°zar) y datasets.
*   üìÇ **`trabajos_practicos/`**: Carpeta centralizada con las entregas acad√©micas (TP1, TP2, TP3).
*   üìÇ **`reports/`**: Informes t√©cnicos en PDF con el an√°lisis detallado de cada desarrollo.

---

## üß† Modelos y Tecnolog√≠as
*   **Arquitecturas**: Word2Vec (CBOW/SkipGram), Redes Neuronales Multicapa (MLP).
*   **Librer√≠as**: TensorFlow/Keras, NumPy, Scikit-learn, Matplotlib, Pillow (GUI).
*   **Generaci√≥n Creativa**: Implementaci√≥n de **Top-k Sampling** para introducir variabilidad literaria y evitar bucles infinitos.

---

## üõ†Ô∏è Instalaci√≥n y Uso

1.  **Clonar el repositorio:**
    ```bash
    git clone https://github.com/tu-usuario/Aprendizaje_Automatico.git
    cd Aprendizaje_Automatico
    ```

2.  **Instalar dependencias:**
    ```bash
    pip install numpy tensorflow scikit-learn matplotlib pillow
    ```

3.  **Ejecutar la Consola de Generaci√≥n:**
    ```bash
    cd 03_Consola_Cortazar_GPT
    python consola.py
    ```

---

## üë• Autores
*   **Seivane Nicol√°s**
*   **Cisnero Mat√≠as**
*   **Serafini Franco**

---
> [!NOTE]
> Este proyecto fue desarrollado bajo la supervisi√≥n acad√©mica de la **Universidad Nacional de Hurlingham**.