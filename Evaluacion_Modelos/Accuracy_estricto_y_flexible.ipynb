{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZAadenQesAc4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "\n",
        "\n",
        "diccionario_palabras = {}\n",
        "diccionario_indices = {}\n",
        "diccionario_onehot = {}\n",
        "diccionario_onehot_a_palabra = {}\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Aprendizaje Automatico Avanzado/corpus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    words = f.read().splitlines()\n",
        "\n",
        "for token in words:\n",
        "    if token not in diccionario_palabras:\n",
        "\n",
        "        index = len(diccionario_palabras)\n",
        "        diccionario_palabras[token] = index\n",
        "        diccionario_indices[index] = token\n",
        "\n",
        "cardinal_V = len(diccionario_palabras)\n",
        "\n",
        "for token, idx in list(diccionario_palabras.items()):\n",
        "\n",
        "    one_hot_vector = np.zeros(cardinal_V)\n",
        "    one_hot_vector[idx] = 1\n",
        "    diccionario_onehot[token] = one_hot_vector\n",
        "    diccionario_onehot_a_palabra[str(one_hot_vector)] = token"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cargar_modelo_completo(nombre_archivo='pesos_cbow_pc2_epoca0.npz'):\n",
        "    \"\"\"\n",
        "    Carga los pesos W1, W2 y los hiperparámetros N, C y eta\n",
        "    desde un archivo .npz.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        data = np.load(nombre_archivo)\n",
        "\n",
        "        W1 = data['W1']\n",
        "        W2 = data['W2']\n",
        "\n",
        "        N = data['N'].item()\n",
        "        C = data['C'].item()\n",
        "        eta = data['eta'].item()\n",
        "\n",
        "        print()\n",
        "\n",
        "        return W1, W2, N, C, eta\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: No se encontró el archivo '{nombre_archivo}'.\")\n",
        "        return None, None, None, None, None\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)"
      ],
      "metadata": {
        "id": "6xpf_LRmsHJI"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W1, W2, N, C, eta = cargar_modelo_completo(\"/content/drive/MyDrive/Aprendizaje Automatico Avanzado/pesos_cbow_pc2_epoca0.npz\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N00sKljb1bMP",
        "outputId": "793c313e-1a2c-4eaf-850e-7143df9aa7a4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predicciones Reales: Donde se propaga como se entrena y se evalua si son, estan las predicciones y los accuracy estrictos y mas variados."
      ],
      "metadata": {
        "id": "NkReQBJ0WTkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predecirCBOW(diccionario_palabras, diccionario_indices, W1, W2, palabras_contexto, MasParecido=None):\n",
        "    # Convertir contexto a índices\n",
        "    contexto = [diccionario_palabras[token] for token in re.findall(r\"\\w+|[.,!?;:]\", palabras_contexto) if token in diccionario_palabras]\n",
        "\n",
        "    if not contexto:\n",
        "        print(\"Las palabras no pertenecen al diccionario.\")\n",
        "        return None, None, None\n",
        "\n",
        "    h = np.mean(W1[contexto], axis=0).reshape(-1, 1)\n",
        "\n",
        "    u = W2.T @ h\n",
        "    y = softmax(u)\n",
        "\n",
        "    if MasParecido is not None and MasParecido > 0:\n",
        "        indices_mas_grandes = np.argsort(y.flatten())[-MasParecido:][::-1]\n",
        "        palabras_resultantes = [diccionario_indices[idx] for idx in indices_mas_grandes]\n",
        "        valores = [y[idx] for idx in indices_mas_grandes]\n",
        "        return indices_mas_grandes, palabras_resultantes, valores\n",
        "    else:\n",
        "        indice_mas_grande = np.argmax(y)\n",
        "        palabra_resultante = diccionario_indices[indice_mas_grande]\n",
        "        valor = y[indice_mas_grande]\n",
        "        return indice_mas_grande, palabra_resultante, valor"
      ],
      "metadata": {
        "id": "_-PcLP6tsJoA"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predecirCBOW(diccionario_palabras, diccionario_indices, W1, W2, palabras_contexto=\"encontraria a la maga? tantas veces me habia bastado asomarme\", MasParecido=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tHFKKZw1WH3",
        "outputId": "d73f580f-161a-4727-fc15-2a3861912943"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([10512, 16271, 19950,  7579,  3275]),\n",
              " ['referente', 'afueras', 'apresurarse', 'montellier', 'analogías'],\n",
              " [array([0.00854287]),\n",
              "  array([0.00495958]),\n",
              "  array([0.00427584]),\n",
              "  array([0.00424342]),\n",
              "  array([0.00390622])])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "151bfe1b",
        "outputId": "8b295015-1785-4f29-f9dc-cad063ca594c"
      },
      "source": [
        "indices_top_5, palabras_top_5, h_vector_top_5 = predecirCBOW(diccionario_palabras, diccionario_indices, W1, W2, palabras_contexto=\"encontraría a la maga? tantas veces me había bastado asomarme\", MasParecido=5)\n",
        "indice_top_1, palabra_top_1, h_vector_top_1 = predecirCBOW(diccionario_palabras, diccionario_indices, W1, W2, palabras_contexto=\"encontraría a la maga? tantas veces me había bastado asomarme\")\n",
        "\n",
        "print(f\"Top 5 palabras: {palabras_top_5}, con valores {h_vector_top_5}\")\n",
        "print(f\"Mejor : {palabra_top_1}, con valores {h_vector_top_1}\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 palabras: ['referente', 'montellier', 'apresurarse', 'afueras', 'limpios'], con valores [array([0.00444491]), array([0.00311397]), array([0.00249189]), array([0.00249071]), array([0.00236007])]\n",
            "Mejor : referente, con valores [0.00444491]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predecirSkipGram(diccionario_palabras, diccionario_indices, W1, W2, palabra_central, contexto=3):\n",
        "    if palabra_central not in diccionario_palabras:\n",
        "        print(\"No está la palabra en el vocabulario\")\n",
        "        return None, None, None\n",
        "\n",
        "    indice = diccionario_palabras[palabra_central]\n",
        "\n",
        "    # Embedding de la palabra central\n",
        "    h = W1[indice].reshape(-1, 1)\n",
        "\n",
        "    # Predicción de contexto\n",
        "    u = W2.T @ h\n",
        "    y = softmax(u)\n",
        "\n",
        "    indices_mas_grandes = np.argsort(y.flatten())[-contexto:][::-1]\n",
        "    palabras_resultantes = [diccionario_indices[idx] for idx in indices_mas_grandes]\n",
        "    valores = [y[idx] for idx in indices_mas_grandes]\n",
        "    return indices_mas_grandes, palabras_resultantes, valores"
      ],
      "metadata": {
        "id": "ak5HGGWK5772"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices_top_5, palabras_top_5, h_vector_top_5 = predecirSkipGram(diccionario_palabras, diccionario_indices, W1, W2,\"había\", contexto=3)\n",
        "indice_top_1, palabra_top_1, h_vector_top_1 = predecirSkipGram(diccionario_palabras, diccionario_indices, W1, W2,\"había\", contexto=10)\n",
        "\n",
        "print(f\"Top 3 words: {palabras_top_5}, con valores {h_vector_top_5}\")\n",
        "print(f\"Top 10 words: {palabra_top_1}, con valores {h_vector_top_1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "B4sGjRM870mW",
        "outputId": "16ed348c-a5b5-4247-b27a-83f448e94eab"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 3 words: ['a', 'la', 'sufriera'], con valores [array([0.00033707]), array([0.00023499]), array([0.00023473])]\n",
            "Top 10 words: ['a', 'la', 'sufriera', 'maga', 'tarde', 'num', 'montellier', 'fundada', 'salgo', 'limpios'], con valores [array([0.00033707]), array([0.00023499]), array([0.00023473]), array([0.00018261]), array([0.00018179]), array([0.00018029]), array([0.00017964]), array([0.0001571]), array([0.00013264]), array([0.00011812])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calcular_error_CBOW(diccionario_palabras, diccionario_indices, W1, W2, corpus, contexto=3, parecidos=3):\n",
        "    indices = [i for i in range(contexto, (len(corpus) - contexto))]\n",
        "    indices_contexto = [i for i in range(-contexto, 0)] + [i for i in range(1, contexto + 1)]\n",
        "    indices_tuplas = [(diccionario_palabras[corpus[i]], [diccionario_palabras[corpus[i+j]] for j in indices_contexto]) for i in indices]\n",
        "\n",
        "    aciertos_estrictos = 0\n",
        "    aciertos_variados = 0\n",
        "\n",
        "    for indice_central, contexto_indices in indices_tuplas:\n",
        "        palabras_contexto = [diccionario_indices[j] for j in contexto_indices]\n",
        "\n",
        "        # Top 1\n",
        "        indice_top_1, _, _ = predecirCBOW(diccionario_palabras, diccionario_indices, W1, W2, \" \".join(palabras_contexto), MasParecido=None)\n",
        "        if indice_top_1 == indice_central:\n",
        "            aciertos_estrictos += 1\n",
        "\n",
        "        # Top k\n",
        "        indices_top_k, _, _ = predecirCBOW(diccionario_palabras, diccionario_indices, W1, W2, \" \".join(palabras_contexto), MasParecido=parecidos)\n",
        "        if indice_central in indices_top_k:\n",
        "            aciertos_variados += 1\n",
        "\n",
        "    accuracy_estricto = aciertos_estrictos / len(indices_tuplas)\n",
        "    accuracy_variado = aciertos_variados / len(indices_tuplas)\n",
        "    return accuracy_estricto, accuracy_variado"
      ],
      "metadata": {
        "id": "aTZURWKP-OUl"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calcular_error_CBOW(diccionario_palabras, diccionario_indices, W1, W2, corpus=words, contexto=3, parecidos=3)"
      ],
      "metadata": {
        "id": "gDw_V3NTO2EE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calcular_error_skipgram(diccionario_palabras, diccionario_indices, W1, W2, corpus, contexto=3):\n",
        "    indices = [i for i in range(contexto, (len(corpus) - contexto))]\n",
        "    indices_contexto = [i for i in range(-contexto, 0)] + [i for i in range(1, contexto + 1)]\n",
        "    indices_tuplas = [(diccionario_palabras[corpus[i]], [diccionario_palabras[corpus[i+j]] for j in indices_contexto]) for i in indices]\n",
        "\n",
        "    aciertos_estrictos = 0\n",
        "    aciertos_variados = 0\n",
        "\n",
        "    for indice_central, contexto_indices in indices_tuplas:\n",
        "        palabra_central = diccionario_indices[indice_central]\n",
        "        palabras_contexto = [diccionario_indices[j] for j in contexto_indices]\n",
        "\n",
        "        _, palabras_top_3, _ = predecirSkipGram(diccionario_palabras, diccionario_indices, W1, W2, palabra_central, contexto=3)\n",
        "\n",
        "        _, palabras_top_10, _ = predecirSkipGram(diccionario_palabras, diccionario_indices, W1, W2, palabra_central, contexto=10)\n",
        "\n",
        "        if all(palabra in palabras_top_3 for palabra in palabras_contexto):\n",
        "            aciertos_estrictos += 1\n",
        "\n",
        "        if any(palabra in palabras_top_10 for palabra in palabras_contexto):\n",
        "            aciertos_variados += 1\n",
        "\n",
        "    accuracy_estricto = aciertos_estrictos / len(indices_tuplas)\n",
        "    accuracy_variado = aciertos_variados / len(indices_tuplas)\n",
        "    return accuracy_estricto, accuracy_variado"
      ],
      "metadata": {
        "id": "HwP6v2IDQHLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calcular_error_skipgram(diccionario_palabras, diccionario_indices, W1, words, contexto=3)"
      ],
      "metadata": {
        "id": "qsj11zPfQ6QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy mas aproximados a los W1, ergo los embeddings"
      ],
      "metadata": {
        "id": "1iLkfKrTWgpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predecirCBOW(diccionario_palabras, diccionario_indices, W1, corpus, palabras_contexto, MasParecido=None):\n",
        "  contexto = [diccionario_palabras[i] for i in re.findall(r\"\\w+|[.,!?;:]\", palabras_contexto)if i in diccionario_palabras]\n",
        "\n",
        "  if not contexto:\n",
        "    print(\"Las palabras no pertenecen al diccionario.\")\n",
        "\n",
        "  return None, None, None\n",
        "\n",
        "  h = np.mean(W1[contexto], axis=0).reshape(-1, 1)\n",
        "\n",
        "  if MasParecido is not None and MasParecido > 0:\n",
        "     indices_mas_grandes = np.argsort(h.flatten())[-MasParecido:][::-1]\n",
        "     palabras_resultantes = [diccionario_indices[indice] for indice in indices_mas_grandes]\n",
        "     valores = [h[i] for i in indices_mas_grandes]\n",
        "     return indices_mas_grandes, palabras_resultantes,valores\n",
        "  else:\n",
        "     indice_mas_grande = np.argmax(h)\n",
        "     palabra_resultante = diccionario_indices[indice_mas_grande]\n",
        "     valores = h[indice_mas_grande]\n",
        "     return indice_mas_grande, palabra_resultante,valores\n",
        "\n",
        "def predecirSkipGram(diccionario_palabras, diccionario_indices, W1, corpus,palabras, contexto=3):\n",
        "\n",
        "  if palabras not in diccionario_palabras:\n",
        "\n",
        "    print(\"No esta la palabra en el vocabulario\")\n",
        "\n",
        "  indice = diccionario_palabras[palabras]\n",
        "  h = W1[indice]\n",
        "  indices_mas_grandes = np.argsort(h.flatten())[-contexto:][::-1]\n",
        "  palabras_resultantes = [diccionario_indices[indice] for indice in indices_mas_grandes]\n",
        "  valores = [h[i] for i in indices_mas_grandes]\n",
        "\n",
        "  return indices_mas_grandes, palabras_resultantes, valores\n",
        "\n",
        "def calcular_error_CBOW(diccionario_palabras, diccionario_indices, W1, corpus, contexto=3, parecidos=3):\n",
        "  indices = [i for i in range(contexto,(len(corpus)-contexto))]\n",
        "  indices_contexto = [i for i in range(-contexto,0)] + [i for i in range(1,contexto+1)]\n",
        "  indices_tuplas = [(diccionario_palabras[corpus[i]], [diccionario_palabras[corpus[i+j]] for j in indices_contexto]) for i in indices]\n",
        "  aciertos_estrictos = 0\n",
        "  aciertos_variados = 0\n",
        "\n",
        "  for i, (indice,contexto)in enumerate(indices_tuplas): palabras_contexto = [diccionario_indices[j] for j in contexto]\n",
        "\n",
        "  indice_top_1, palabra_top_1, h_vector_top_1 = predecirCBOW(diccionario_palabras, diccionario_indices, W1, words, \" \".join(palabras_contexto), MasParecido=None)\n",
        "\n",
        "  if indice_top_1 == indice:\n",
        "     aciertos_estrictos += 1\n",
        "  indice_top_varias, palabra_top_varias, h_vector_top_varias = predecirCBOW(diccionario_palabras, diccionario_indices, W1, words, \" \".join(palabras_contexto), MasParecido=parecidos)\n",
        "\n",
        "  if indice in indice_top_varias:\n",
        "     aciertos_variados += 1\n",
        "  accuracy_estricto = aciertos_estrictos / len(indices_tuplas)\n",
        "  accuracy_variado = aciertos_variados / len(indices_tuplas)\n",
        "\n",
        "  return accuracy_estricto, accuracy_variado\n",
        "\n",
        "def calcular_error_skipgram(diccionario_palabras, diccionario_indices, W1, corpus, contexto=3):\n",
        "   indices = [i for i in range(contexto,(len(corpus)-contexto))]\n",
        "   indices_contexto = [i for i in range(-contexto,0)] + [i for i in range(1,contexto+1)]\n",
        "   indices_tuplas = [(diccionario_palabras[corpus[i]], [diccionario_palabras[corpus[i+j]] for j in indices_contexto]) for i in indices]\n",
        "\n",
        "   aciertos_estrictos = 0\n",
        "   aciertos_variados = 0\n",
        "\n",
        "   for i, (indice,contexto)in enumerate(indices_tuplas):\n",
        "     palabra = diccionario_indices[indice]\n",
        "     palabras_contexto_indices = contexto\n",
        "     palabras_contexto = [diccionario_indices[j] for j in palabras_contexto_indices]\n",
        "     indices_top_3, palabras_top_3, h_vector_top_3 = predecirSkipGram(diccionario_palabras, diccionario_indices, W1, words,palabra, contexto=3)\n",
        "     indices_top_10, palabra_top_10, h_vector_top_10 = predecirSkipGram(diccionario_palabras, diccionario_indices, W1, words,palabra, contexto=10)\n",
        "\n",
        "     if all(palabra_contexto in palabras_top_3 for palabra_contexto in palabras_contexto):\n",
        "      aciertos_estrictos += 1\n",
        "\n",
        "     if any(palabra_contexto in palabra_top_10 for palabra_contexto in palabras_contexto):\n",
        "      aciertos_variados += 1\n",
        "\n",
        "     accuracy_estricto = aciertos_estrictos / len(indices_tuplas)\n",
        "     accuracy_variado = aciertos_variados / len(indices_tuplas)\n",
        "\n",
        "\n",
        "     return accuracy_estricto, accuracy_variado"
      ],
      "metadata": {
        "id": "Qfs9NQHCU2m7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}